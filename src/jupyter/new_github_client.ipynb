{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8dde8d1-a6cb-49b1-8741-6fadf4325e1c",
   "metadata": {},
   "source": [
    "### Homework: 尝试扩展 GitHubClient，使其支持 Until 参数条件筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19bcfb3e-ed56-4de6-b2bd-11c1a12961eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "from loguru import logger\n",
    "import sys\n",
    "# Configure Loguru\n",
    "logger.remove()  # Remove the default logger\n",
    "logger.add(sys.stdout, level=\"DEBUG\", format=\"{time} {level} {message}\", colorize=True)\n",
    "logger.add(\"logs/app.log\", rotation=\"1 MB\", level=\"DEBUG\")\n",
    "# Alias the logger for easier import\n",
    "LOG = logger\n",
    "# Make the logger available for import with the alias\n",
    "__all__ = [\"LOG\"]\n",
    "# src/github_client.py\n",
    "import requests  # 导入requests库用于HTTP请求\n",
    "from datetime import datetime, date, timedelta  # 导入日期处理模块\n",
    "import os  # 导入os模块用于文件和目录操作\n",
    "# from logger import LOG  # 导入日志模块（演示时直接导入）\n",
    "\n",
    "class GitHubClient:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.headers = {'Authorization': f'token {self.token}'}\n",
    "\n",
    "    def fetch_updates(self, repo, since=None, until=None):\n",
    "        \"\"\"\n",
    "        获取指定仓库在给定时间范围内的更新\n",
    "        \n",
    "        Args:\n",
    "            repo (str): 仓库名称 (格式: \"owner/repo\")\n",
    "            since (str): 起始时间 (ISO 8601格式)\n",
    "            until (str): 结束时间 (ISO 8601格式)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            updates = {\n",
    "                'commits': self.fetch_commits(repo, since, until),\n",
    "                'issues': self.fetch_issues(repo, since, until),\n",
    "                'pull_requests': self.fetch_pull_requests(repo, since, until)\n",
    "            }\n",
    "            LOG.info(f\"Successfully fetched updates for {repo} from {since} to {until}\")\n",
    "            return updates\n",
    "        except Exception as e:\n",
    "            LOG.error(f\"Error fetching updates for {repo}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _filter_by_date(self, items, until_date):\n",
    "        \"\"\"\n",
    "        根据until日期过滤项目列表\n",
    "        \n",
    "        Args:\n",
    "            items (list): 要过滤的项目列表\n",
    "            until_date (str): 结束日期 (ISO 8601格式)\n",
    "        \"\"\"\n",
    "        if not until_date:\n",
    "            return items\n",
    "            \n",
    "        until_datetime = datetime.fromisoformat(until_date.replace('Z', '+00:00'))\n",
    "        return [\n",
    "            item for item in items \n",
    "            if datetime.fromisoformat(item.get('created_at', '').replace('Z', '+00:00')) <= until_datetime\n",
    "        ]\n",
    "\n",
    "    def fetch_commits(self, repo, since=None, until=None):\n",
    "        \"\"\"获取提交记录\"\"\"\n",
    "        url = f'https://api.github.com/repos/{repo}/commits'\n",
    "        params = {'per_page': 100}  # 增加每页数量\n",
    "        \n",
    "        if since:\n",
    "            params['since'] = since\n",
    "        if until:\n",
    "            params['until'] = until\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            commits = response.json()\n",
    "            LOG.debug(f\"Fetched {len(commits)} commits for {repo}\")\n",
    "            return commits\n",
    "        except Exception as e:\n",
    "            LOG.error(f\"Error fetching commits for {repo}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_issues(self, repo, since=None, until=None):\n",
    "        \"\"\"获取问题\"\"\"\n",
    "        url = f'https://api.github.com/repos/{repo}/issues'\n",
    "        params = {\n",
    "            'state': 'closed',\n",
    "            'per_page': 100,\n",
    "            'sort': 'updated',\n",
    "            'direction': 'desc'\n",
    "        }\n",
    "        \n",
    "        if since:\n",
    "            params['since'] = since\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            issues = response.json()\n",
    "            \n",
    "            # 使用until参数过滤\n",
    "            if until:\n",
    "                issues = self._filter_by_date(issues, until)\n",
    "                \n",
    "            LOG.debug(f\"Fetched {len(issues)} issues for {repo}\")\n",
    "            return issues\n",
    "        except Exception as e:\n",
    "            LOG.error(f\"Error fetching issues for {repo}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def fetch_pull_requests(self, repo, since=None, until=None):\n",
    "        \"\"\"获取拉取请求\"\"\"\n",
    "        url = f'https://api.github.com/repos/{repo}/pulls'\n",
    "        params = {\n",
    "            'state': 'closed',\n",
    "            'per_page': 100,\n",
    "            'sort': 'updated',\n",
    "            'direction': 'desc'\n",
    "        }\n",
    "        \n",
    "        if since:\n",
    "            params['since'] = since\n",
    "            \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            pull_requests = response.json()\n",
    "            \n",
    "            # 使用until参数过滤\n",
    "            if until:\n",
    "                pull_requests = self._filter_by_date(pull_requests, until)\n",
    "                \n",
    "            LOG.debug(f\"Fetched {len(pull_requests)} pull requests for {repo}\")\n",
    "            return pull_requests\n",
    "        except Exception as e:\n",
    "            LOG.error(f\"Error fetching pull requests for {repo}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345d7a6e-34b9-403b-b52c-47c230f4b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15T14:38:57.146289+0800 DEBUG Fetched 100 commits for langchain-ai/langchain\n",
      "2024-11-15T14:38:59.399125+0800 DEBUG Fetched 100 issues for langchain-ai/langchain\n",
      "2024-11-15T14:39:02.385410+0800 DEBUG Fetched 100 pull requests for langchain-ai/langchain\n",
      "2024-11-15T14:39:02.387652+0800 INFO Successfully fetched updates for langchain-ai/langchain from 2024-11-01T00:00:00Z to 2024-11-15T23:59:59Z\n",
      "Commit: ef2dc9e - docs: update \"quickstart\" tutorial (#28096)\n",
      "\n",
      "- Update language / add links in places\n",
      "- De-emphasize output parsers\n",
      "- remove deployment section\n",
      "Commit: f122273 - core[patch]: support numpy 2 (#27991)\n",
      "Commit: cff70c2 - docs: Add hyperlink to immediately show the table at the bottom of th… (#28102)\n",
      "\n",
      "Added a hyperlink which can be clicked so users can immediately see the\n",
      "table and find out the various example selector methods\n",
      "Commit: 4b641f8 - English Update and fixed a duplicate \"the\" (#27981)\n",
      "\n",
      "Fixed a duplicate \"the\" in the documentation and made the documentation\n",
      "generally easier to understand\n",
      "Commit: f6d3458 - docs: throw on broken anchors (#27773)\n",
      "\n",
      "Co-authored-by: Eugene Yurtsev <eyurtsev@gmail.com>\n",
      "Commit: 7bd9c8c - docs: Updated link to ensure reference to the correct header for ToolNode (#28088)\n",
      "\n",
      "When `ToolNode` hyperlink is clicked, it does not automatically scroll\n",
      "to the section due to incorrect reference to the heading / id in the\n",
      "LangGraph documentation\n",
      "Commit: 940e93e - docs: add docs on StrOutputParser (#28089)\n",
      "\n",
      "Think it's worth adding a quick guide and including in the table in the\n",
      "concepts page. `StrOutputParser` can make it easier to deal with the\n",
      "union type for message content. For example, ChatAnthropic with bound\n",
      "tools will generate string content if there are no tool calls and\n",
      "`list[dict]` content otherwise.\n",
      "\n",
      "I'm also considering removing the output parser section from the\n",
      "[\"quickstart\"\n",
      "tutorial](https://python.langchain.com/docs/tutorials/llm_chain/); we\n",
      "can link to this guide instead.\n",
      "Commit: 6ec688c - xai[patch]: update core (#28092)\n",
      "Commit: 2ab5673 - docs: Add example using `TypedDict` in structured outputs how-to guide (#27415)\n",
      "\n",
      "For me, the [Pydantic\n",
      "example](https://python.langchain.com/docs/how_to/structured_output/#choosing-between-multiple-schemas)\n",
      "does not work (tested on various Python versions from 3.10 to 3.12, and\n",
      "`Pydantic` versions from 2.7 to 2.9).\n",
      "\n",
      "The `TypedDict` example (added in this PR) does.\n",
      "\n",
      "----\n",
      "\n",
      "Additionally, fixed an error in [Using PydanticOutputParser\n",
      "example](https://python.langchain.com/docs/how_to/structured_output/#using-pydanticoutputparser).\n",
      "\n",
      "Was:\n",
      "\n",
      "```python\n",
      "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
      "\n",
      "print(prompt.invoke(query).to_string())\n",
      "```\n",
      "\n",
      "Corrected to:\n",
      "\n",
      "```python\n",
      "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
      "\n",
      "print(prompt.invoke({\"query\": query}).to_string())\n",
      "```\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Eugene Yurtsev <eyurtsev@gmail.com>\n",
      "Commit: 3e972fa - community: chore warn deprecate the tracer (#27159)\n",
      "\n",
      "- **Description:**: This PR deprecates the wandb tracer in favor of the\n",
      "new\n",
      "[WeaveTracer](https://weave-docs.wandb.ai/guides/integrations/langchain#using-weavetracer)\n",
      "in W&B\n",
      "- **Dependencies:** No dependencies, just a deprecation warning.\n",
      "- **Twitter handle:** @parambharat\n",
      "\n",
      "\n",
      "@baskaryan\n",
      "Commit: 76e0127 - core: release 0.3.18 (#28070)\n",
      "Commit: eadc2f6 - core: added DeleteResponse to the module (#28069)\n",
      "\n",
      "Description:\n",
      "* added `DeleteResponse` to the `langchain_core.indexing` module, for\n",
      "implementing DocumentIndex classes.\n",
      "Commit: c89e7ce - core[patch]: Update doc-strings in callbacks (#28073)\n",
      "\n",
      "- Fix api docs\n",
      "Commit: 965286d - docs: fix spelling error (#28075)\n",
      "\n",
      "Fix spelling error in docs\n",
      "Commit: 892694d - docs: Fixed broken link for AI models introduction (#28079)\n",
      "\n",
      "Fixed broken redirect to the introduction to AI models in the Forefront\n",
      "platform\n",
      "Commit: beef4c4 - Proofreading and Editing Report for Migration Guide (#28084)\n",
      "\n",
      "Corrections and Suggestions for Migrating LangChain Code Documentation\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- [ ] **PR title**: \"package: description\"\n",
      "- Where \"package\" is whichever of langchain, community, core, etc. is\n",
      "being modified. Use \"docs: ...\" for purely docs changes, \"infra: ...\"\n",
      "for CI changes.\n",
      "  - Example: \"community: add foobar LLM\"\n",
      "\n",
      "\n",
      "- [ ] **PR message**: ***Delete this entire checklist*** and replace\n",
      "with\n",
      "    - **Description:** a description of the change\n",
      "    - **Issue:** the issue # it fixes, if applicable\n",
      "    - **Dependencies:** any dependencies required for this change\n",
      "- **Twitter handle:** if your PR gets announced, and you'd like a\n",
      "mention, we'll gladly shout you out!\n",
      "\n",
      "\n",
      "- [ ] **Add tests and docs**: If you're adding a new integration, please\n",
      "include\n",
      "1. a test for the integration, preferably unit tests that do not rely on\n",
      "network access,\n",
      "2. an example notebook showing its use. It lives in\n",
      "`docs/docs/integrations` directory.\n",
      "\n",
      "\n",
      "- [ ] **Lint and test**: Run `make format`, `make lint` and `make test`\n",
      "from the root of the package(s) you've modified. See contribution\n",
      "guidelines for more: https://python.langchain.com/docs/contributing/\n",
      "\n",
      "Additional guidelines:\n",
      "- Make sure optional dependencies are imported within a function.\n",
      "- Please do not add dependencies to pyproject.toml files (even optional\n",
      "ones) unless they are required for unit tests.\n",
      "- Most PRs should not touch more than one package.\n",
      "- Changes should be backwards compatible.\n",
      "- If you are adding something to community, do not re-import it in\n",
      "langchain.\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "Commit: 2cec957 - docs: Fix missing space between the words API Reference (#28087)\n",
      "\n",
      "Added an expected space between the words APIReference\n",
      "Commit: da7c79b - DOCS: Concept Section Improvements & Updates (#27733)\n",
      "\n",
      "Edited mainly the `Concepts` section in the LangChain documentation.\n",
      "\n",
      "Overview:\n",
      "* Updated some explanations to make the point more clear / Add missing\n",
      "words for some documentations.\n",
      "* Rephrased some sentences to make it shorter and more concise.\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Eugene Yurtsev <eyurtsev@gmail.com>\n",
      "Co-authored-by: Eugene Yurtsev <eugene@langchain.dev>\n",
      "Commit: 02de346 - docs: Fixed additional 'the' and remove 'turns' to make explanation clearer (#28082)\n",
      "\n",
      "Fixed additional 'the' and remove the word 'turns' as it would make\n",
      "explanation clearer\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: ccurme <chester.curme@gmail.com>\n",
      "Commit: 298ebee - docs: Fixed broken link for Cloudfare docs for the models available (#28080)\n",
      "\n",
      "Fixed the broken redirect to see all the cloudfare models\n",
      "Commit: 8241c0d - docs: Fixed wrong link redirect from JS ToolMessage to Python ToolMes… (#28083)\n",
      "\n",
      "Fixed the link to ToolMessage from the JS documentation to Python\n",
      "documentation\n",
      "Commit: 77c8a5c - docs: Fixed broken link to the Luminous model family introduction (#28078)\n",
      "\n",
      "The Luminous Model hyperlink at the start of the model is broken.\n",
      "Fixed it to update it with the latest link used by the integration\n",
      "Commit: 09e85c7 - xai[patch]: update dependencies (#28067)\n",
      "Commit: a646f1c - Handled empty search result handling and updated the notebook (#27914)\n",
      "\n",
      "- [ ] **PR title**: \"community: updated Kinetica vectorstore\"\n",
      "\n",
      "  - **Description:** Handled empty search results\n",
      "  - **Issue:** used to throw error if the search results were empty\n",
      "\n",
      "@efriis\n",
      "Commit: 00e7b2d - anthropic[patch]: add examples to API ref (#28065)\n",
      "Commit: 48ee322 - partners: add xAI chat integration (#28032)\n",
      "Commit: 2898b95 - anthropic[major]: release 0.3.0 (#28063)\n",
      "Commit: 5eaa0e8 - openai[patch]: release 0.2.8 (#28062)\n",
      "Commit: 15b7dd3 - community[patch]: release 0.3.7 (#28061)\n",
      "Commit: 5460096 - core[patch]: release 0.3.17 (#28060)\n",
      "Commit: 1538ee1 - anthropic[major]: support python 3.13 (#27916)\n",
      "\n",
      "Last week Anthropic released version 0.39.0 of its python sdk, which\n",
      "enabled support for Python 3.13. This release deleted a legacy\n",
      "`client.count_tokens` method, which we currently access during init of\n",
      "the `Anthropic` LLM. Anthropic has replaced this functionality with the\n",
      "[client.beta.messages.count_tokens()\n",
      "API](https://github.com/anthropics/anthropic-sdk-python/pull/726).\n",
      "\n",
      "To enable support for `anthropic >= 0.39.0` and Python 3.13, here we\n",
      "drop support for the legacy token counting method, and add support for\n",
      "the new method via `ChatAnthropic.get_num_tokens_from_messages`.\n",
      "\n",
      "To fully support the token counting API, we update the signature of\n",
      "`get_num_tokens_from_message` to accept tools everywhere.\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Bagatur <22008038+baskaryan@users.noreply.github.com>\n",
      "Commit: 759b6ed - docs: Fix typo in Tavily Search example (#28034)\n",
      "\n",
      "Changed \"demon\" to \"demo\" in the code comment for clarity.\n",
      "\n",
      "PR Title\n",
      "docs: Fix typo in Tavily Search example\n",
      "\n",
      "PR Message\n",
      "Description:\n",
      "This PR fixes a typo in the code comment of the Tavily Search\n",
      "documentation. Changed \"demon\" to \"demo\" for clarity and to avoid\n",
      "confusion.\n",
      "\n",
      "Issue:\n",
      "No specific issue was mentioned, but this is a minor improvement in\n",
      "documentation.\n",
      "\n",
      "Dependencies:\n",
      "No additional dependencies required.\n",
      "Commit: ca7375a - Improvement[Community]Improve Embeddings API (#28038)\n",
      "\n",
      "- Fix `BaichuanTextEmbeddings` api url\n",
      "- Remove unused params in api doc\n",
      "- Fix word spelling\n",
      "Commit: e290736 - Update streaming.mdx (#28055)\n",
      "\n",
      "fix: correct grammar in documentation for streaming modes\n",
      "\n",
      "Updated sentence to clarify usage of \"choose\" in \"When using the stream\n",
      "and astream methods with LangGraph, you can choose one or more streaming\n",
      "modes...\" for better readability.\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- [ ] **PR title**: \"package: description\"\n",
      "- Where \"package\" is whichever of langchain, community, core, etc. is\n",
      "being modified. Use \"docs: ...\" for purely docs changes, \"infra: ...\"\n",
      "for CI changes.\n",
      "  - Example: \"community: add foobar LLM\"\n",
      "\n",
      "\n",
      "- [ ] **PR message**: ***Delete this entire checklist*** and replace\n",
      "with\n",
      "    - **Description:** a description of the change\n",
      "    - **Issue:** the issue # it fixes, if applicable\n",
      "    - **Dependencies:** any dependencies required for this change\n",
      "- **Twitter handle:** if your PR gets announced, and you'd like a\n",
      "mention, we'll gladly shout you out!\n",
      "\n",
      "\n",
      "- [ ] **Add tests and docs**: If you're adding a new integration, please\n",
      "include\n",
      "1. a test for the integration, preferably unit tests that do not rely on\n",
      "network access,\n",
      "2. an example notebook showing its use. It lives in\n",
      "`docs/docs/integrations` directory.\n",
      "\n",
      "\n",
      "- [ ] **Lint and test**: Run `make format`, `make lint` and `make test`\n",
      "from the root of the package(s) you've modified. See contribution\n",
      "guidelines for more: https://python.langchain.com/docs/contributing/\n",
      "\n",
      "Additional guidelines:\n",
      "- Make sure optional dependencies are imported within a function.\n",
      "- Please do not add dependencies to pyproject.toml files (even optional\n",
      "ones) unless they are required for unit tests.\n",
      "- Most PRs should not touch more than one package.\n",
      "- Changes should be backwards compatible.\n",
      "- If you are adding something to community, do not re-import it in\n",
      "langchain.\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "Commit: f9212c7 - DOC: Fix typo in documentation for streaming modes, correcting 'witte… (#28052)\n",
      "\n",
      "…n' to 'written' in 'Emit custom output written using LangGraph’s\n",
      "StreamWriter.'\n",
      "\n",
      "### Changes:\n",
      "- Corrected the typo in the phrase 'Emit custom output witten using\n",
      "LangGraph’s StreamWriter.' to 'Emit custom output written using\n",
      "LangGraph’s StreamWriter.'\n",
      "- Enhanced the clarity of the documentation surrounding LangGraph’s\n",
      "streaming modes, specifically around the StreamWriter functionality.\n",
      "- Provided additional context and emphasis on the role of the\n",
      "StreamWriter class in handling custom output.\n",
      "\n",
      "### Issue Reference:\n",
      "- GitHub issue: https://github.com/langchain-ai/langchain/issues/28051\n",
      "\n",
      "This update addresses the issue raised regarding the incorrect spelling\n",
      "and aims to improve the clarity of the streaming mode documentation for\n",
      "better user understanding.\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- [ ] **PR title**: \"package: description\"\n",
      "- Where \"package\" is whichever of langchain, community, core, etc. is\n",
      "being modified. Use \"docs: ...\" for purely docs changes, \"infra: ...\"\n",
      "for CI changes.\n",
      "  - Example: \"community: add foobar LLM\"\n",
      "\n",
      "\n",
      "- [ ] **PR message**:\n",
      "**Description:**  \n",
      "Fixed a typo in the documentation for streaming modes, changing \"witten\"\n",
      "to \"written\" in the phrase \"Emit custom output witten using LangGraph’s\n",
      "StreamWriter.\"\n",
      "**Issue:**  \n",
      "This PR addresses and fixes the typo in the documentation referenced in\n",
      "[#28051](https://github.com/langchain-ai/langchain/issues/28051).\n",
      "\n",
      "\n",
      "**Issue:**  \n",
      "This PR addresses and fixes the typo in the documentation referenced in\n",
      "[#28051](https://github.com/langchain-ai/langchain/issues/28051).\n",
      "\n",
      "\n",
      "- [ ] **Add tests and docs**: If you're adding a new integration, please\n",
      "include\n",
      "1. a test for the integration, preferably unit tests that do not rely on\n",
      "network access,\n",
      "2. an example notebook showing its use. It lives in\n",
      "`docs/docs/integrations` directory.\n",
      "\n",
      "\n",
      "- [ ] **Lint and test**: Run `make format`, `make lint` and `make test`\n",
      "from the root of the package(s) you've modified. See contribution\n",
      "guidelines for more: https://python.langchain.com/docs/contributing/\n",
      "\n",
      "Additional guidelines:\n",
      "- Make sure optional dependencies are imported within a function.\n",
      "- Please do not add dependencies to pyproject.toml files (even optional\n",
      "ones) unless they are required for unit tests.\n",
      "- Most PRs should not touch more than one package.\n",
      "- Changes should be backwards compatible.\n",
      "- If you are adding something to community, do not re-import it in\n",
      "langchain.\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "Commit: 139881b - openai[patch]: fix azure oai stream check (#28048)\n",
      "Commit: 9611f0b - openai[patch]: Release 0.2.7 (#28047)\n",
      "Commit: 5c14e1f - community[patch]: Release 0.3.6 (#28046)\n",
      "Commit: 9ebd7eb - core[patch]: Release 0.3.16 (#28045)\n",
      "Commit: 9484cc0 - community[docs]: modify parameter for the LoRA adapter on the vllm page (#27930)\n",
      "\n",
      "**Description:** \n",
      "This PR modifies the documentation regarding the configuration of the\n",
      "VLLM with the LoRA adapter. The updates aim to provide clear\n",
      "instructions for users on how to set up the LoRA adapter when using the\n",
      "VLLM.\n",
      "\n",
      "- before\n",
      "```python\n",
      "VLLM(..., enable_lora=True)\n",
      "```\n",
      "- after\n",
      "```python\n",
      "VLLM(..., \n",
      "    vllm_kwargs={\n",
      "        \"enable_lora\": True\n",
      "    }\n",
      ")\n",
      "```\n",
      "This change clarifies that users should use the vllm_kwargs to enable\n",
      "the LoRA adapter.\n",
      "\n",
      "Co-authored-by: Um Changyong <changyong.um@sfa.co.kr>\n",
      "Commit: 0b85f90 - docs: Makes the phrasing more smooth and reasoning more clear (#28020)\n",
      "\n",
      "Updated the phrasing and reasoning on the \"abstraction not receiving\n",
      "much development\" part of the documentation\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: ccurme <chester.curme@gmail.com>\n",
      "Commit: f695b96 - docs:Fixed missing hyperlink and changed AI to LLMs for clarity (#28006)\n",
      "\n",
      "Changed \"AI\" to \"LLM\" in a paragraph\n",
      "Fixed missing hyperlink for the structured output point\n",
      "Commit: c0f3777 - docs: removed bolding from header (#28001)\n",
      "\n",
      "removed extra ** after heading two\n",
      "Commit: 44df79c - Correcting AzureOpenAI initialization (#28014)\n",
      "Commit: 57fc623 - docs : Update sql_qa.ipynb (#28026)\n",
      "\n",
      "Text Documentation Bug:\n",
      "\n",
      "Changed DSL query to SQL query.\n",
      "Commit: 922b6b0 - docs: update some cassettes (#28010)\n",
      "Commit: 8e91c7c - docs: add cross-links (#28000)\n",
      "\n",
      "Mainly to improve visibility of integration pages.\n",
      "Commit: 33dbfba - openai[patch]: default to invoke on o1 stream() (#27983)\n",
      "Commit: 503f248 - docs: intro nit (#27998)\n",
      "Commit: ff2152b - docs: update tutorials index and add get started guides (#27996)\n",
      "Commit: c421997 - community[patch]: Added type hinting to OpenSearch clients (#27946)\n",
      "\n",
      "Description:\n",
      "* When working with OpenSearchVectorSearch to make\n",
      "OpenSearchGraphVectorStore (coming soon), I noticed that there wasn't\n",
      "type hinting for the underlying OpenSearch clients. This fixes that\n",
      "issue.\n",
      "* Confirmed tests are still passing with code changes.\n",
      "\n",
      "Note that there is some additional code duplication now, but I think\n",
      "this approach is cleaner overall.\n",
      "Commit: 4c2392e - docs: fix link in custom tools guide (#27975)\n",
      "\n",
      "Fixed broken link in tools documentation for `BaseTool`\n",
      "Commit: 85925e3 - docs: fix link in tool-calling guide (#27976)\n",
      "\n",
      "Fix broken BaseTool link in documentation\n",
      "Commit: 138f360 - docs: fix typo in PDF loader guide (#27977)\n",
      "\n",
      "Fixed duplicate \"py\" in hyperlink to `pypdf` docs\n",
      "Commit: b509747 - Community: Google Books API Tool (#27307)\n",
      "\n",
      "## Description\n",
      "\n",
      "As proposed in our earlier discussion #26977 we have introduced a Google\n",
      "Books API Tool that leverages the Google Books API found at\n",
      "[https://developers.google.com/books/docs/v1/using](https://developers.google.com/books/docs/v1/using)\n",
      "to generate book recommendations.\n",
      "\n",
      "### Sample Usage\n",
      "\n",
      "```python\n",
      "from langchain_community.tools import GoogleBooksQueryRun\n",
      "from langchain_community.utilities import GoogleBooksAPIWrapper\n",
      "\n",
      "api_wrapper = GoogleBooksAPIWrapper()\n",
      "tool = GoogleBooksQueryRun(api_wrapper=api_wrapper)\n",
      "\n",
      "tool.run('ai')\n",
      "```\n",
      "\n",
      "### Sample Output\n",
      "\n",
      "```txt\n",
      "Here are 5 suggestions based off your search for books related to ai:\n",
      "\n",
      "1. \"AI's Take on the Stigma Against AI-Generated Content\" by Sandy Y. Greenleaf: In a world where artificial intelligence (AI) is rapidly advancing and transforming various industries, a new form of content creation has emerged: AI-generated content. However, despite its potential to revolutionize the way we produce and consume information, AI-generated content often faces a significant stigma. \"AI's Take on the Stigma Against AI-Generated Content\" is a groundbreaking book that delves into the heart of this issue, exploring the reasons behind the stigma and offering a fresh, unbiased perspective on the topic. Written from the unique viewpoint of an AI, this book provides readers with a comprehensive understanding of the challenges and opportunities surrounding AI-generated content. Through engaging narratives, thought-provoking insights, and real-world examples, this book challenges readers to reconsider their preconceptions about AI-generated content. It explores the potential benefits of embracing this technology, such as increased efficiency, creativity, and accessibility, while also addressing the concerns and drawbacks that contribute to the stigma. As you journey through the pages of this book, you'll gain a deeper understanding of the complex relationship between humans and AI in the realm of content creation. You'll discover how AI can be used as a tool to enhance human creativity, rather than replace it, and how collaboration between humans and machines can lead to unprecedented levels of innovation. Whether you're a content creator, marketer, business owner, or simply someone curious about the future of AI and its impact on our society, \"AI's Take on the Stigma Against AI-Generated Content\" is an essential read. With its engaging writing style, well-researched insights, and practical strategies for navigating this new landscape, this book will leave you equipped with the knowledge and tools needed to embrace the AI revolution and harness its potential for success. Prepare to have your assumptions challenged, your mind expanded, and your perspective on AI-generated content forever changed. Get ready to embark on a captivating journey that will redefine the way you think about the future of content creation.\n",
      "Read more at https://play.google.com/store/books/details?id=4iH-EAAAQBAJ&source=gbs_api\n",
      "\n",
      "2. \"AI Strategies For Web Development\" by Anderson Soares Furtado Oliveira: From fundamental to advanced strategies, unlock useful insights for creating innovative, user-centric websites while navigating the evolving landscape of AI ethics and security Key Features Explore AI's role in web development, from shaping projects to architecting solutions Master advanced AI strategies to build cutting-edge applications Anticipate future trends by exploring next-gen development environments, emerging interfaces, and security considerations in AI web development Purchase of the print or Kindle book includes a free PDF eBook Book Description If you're a web developer looking to leverage the power of AI in your projects, then this book is for you. Written by an AI and ML expert with more than 15 years of experience, AI Strategies for Web Development takes you on a transformative journey through the dynamic intersection of AI and web development, offering a hands-on learning experience.The first part of the book focuses on uncovering the profound impact of AI on web projects, exploring fundamental concepts, and navigating popular frameworks and tools. As you progress, you'll learn how to build smart AI applications with design intelligence, personalized user journeys, and coding assistants. Later, you'll explore how to future-proof your web development projects using advanced AI strategies and understand AI's impact on jobs. Toward the end, you'll immerse yourself in AI-augmented development, crafting intelligent web applications and navigating the ethical landscape.Packed with insights into next-gen development environments, AI-augmented practices, emerging realities, interfaces, and security governance, this web development book acts as your roadmap to staying ahead in the AI and web development domain. What you will learn Build AI-powered web projects with optimized models Personalize UX dynamically with AI, NLP, chatbots, and recommendations Explore AI coding assistants and other tools for advanced web development Craft data-driven, personalized experiences using pattern recognition Architect effective AI solutions while exploring the future of web development Build secure and ethical AI applications following TRiSM best practices Explore cutting-edge AI and web development trends Who this book is for This book is for web developers with experience in programming languages and an interest in keeping up with the latest trends in AI-powered web development. Full-stack, front-end, and back-end developers, UI/UX designers, software engineers, and web development enthusiasts will also find valuable information and practical guidelines for developing smarter websites with AI. To get the most out of this book, it is recommended that you have basic knowledge of programming languages such as HTML, CSS, and JavaScript, as well as a familiarity with machine learning concepts.\n",
      "Read more at https://play.google.com/store/books/details?id=FzYZEQAAQBAJ&source=gbs_api\n",
      "\n",
      "3. \"Artificial Intelligence for Students\" by Vibha Pandey: A multifaceted approach to develop an understanding of AI and its potential applications KEY FEATURES ● AI-informed focuses on AI foundation, applications, and methodologies. ● AI-inquired focuses on computational thinking and bias awareness. ● AI-innovate focuses on creative and critical thinking and the Capstone project. DESCRIPTION AI is a discipline in Computer Science that focuses on developing intelligent machines, machines that can learn and then teach themselves. If you are interested in AI, this book can definitely help you prepare for future careers in AI and related fields. The book is aligned with the CBSE course, which focuses on developing employability and vocational competencies of students in skill subjects. The book is an introduction to the basics of AI. It is divided into three parts – AI-informed, AI-inquired and AI-innovate. It will help you understand AI's implications on society and the world. You will also develop a deeper understanding of how it works and how it can be used to solve complex real-world problems. Additionally, the book will also focus on important skills such as problem scoping, goal setting, data analysis, and visualization, which are essential for success in AI projects. Lastly, you will learn how decision trees, neural networks, and other AI concepts are commonly used in real-world applications. By the end of the book, you will develop the skills and competencies required to pursue a career in AI. WHAT YOU WILL LEARN ● Get familiar with the basics of AI and Machine Learning. ● Understand how and where AI can be applied. ● Explore different applications of mathematical methods in AI. ● Get tips for improving your skills in Data Storytelling. ● Understand what is AI bias and how it can affect human rights. WHO THIS BOOK IS FOR This book is for CBSE class XI and XII students who want to learn and explore more about AI. Basic knowledge of Statistical concepts, Algebra, and Plotting of equations is a must. TABLE OF CONTENTS 1. Introduction: AI for Everyone 2. AI Applications and Methodologies 3. Mathematics in Artificial Intelligence 4. AI Values (Ethical Decision-Making) 5. Introduction to Storytelling 6. Critical and Creative Thinking 7. Data Analysis 8. Regression 9. Classification and Clustering 10. AI Values (Bias Awareness) 11. Capstone Project 12. Model Lifecycle (Knowledge) 13. Storytelling Through Data 14. AI Applications in Use in Real-World\n",
      "Read more at https://play.google.com/store/books/details?id=ptq1EAAAQBAJ&source=gbs_api\n",
      "\n",
      "4. \"The AI Book\" by Ivana Bartoletti, Anne Leslie and Shân M. Millie: Written by prominent thought leaders in the global fintech space, The AI Book aggregates diverse expertise into a single, informative volume and explains what artifical intelligence really means and how it can be used across financial services today. Key industry developments are explained in detail, and critical insights from cutting-edge practitioners offer first-hand information and lessons learned. Coverage includes: · Understanding the AI Portfolio: from machine learning to chatbots, to natural language processing (NLP); a deep dive into the Machine Intelligence Landscape; essentials on core technologies, rethinking enterprise, rethinking industries, rethinking humans; quantum computing and next-generation AI · AI experimentation and embedded usage, and the change in business model, value proposition, organisation, customer and co-worker experiences in today’s Financial Services Industry · The future state of financial services and capital markets – what’s next for the real-world implementation of AITech? · The innovating customer – users are not waiting for the financial services industry to work out how AI can re-shape their sector, profitability and competitiveness · Boardroom issues created and magnified by AI trends, including conduct, regulation & oversight in an algo-driven world, cybersecurity, diversity & inclusion, data privacy, the ‘unbundled corporation’ & the future of work, social responsibility, sustainability, and the new leadership imperatives · Ethical considerations of deploying Al solutions and why explainable Al is so important\n",
      "Read more at http://books.google.ca/books?id=oE3YDwAAQBAJ&dq=ai&hl=&source=gbs_api\n",
      "\n",
      "5. \"Artificial Intelligence in Society\" by OECD: The artificial intelligence (AI) landscape has evolved significantly from 1950 when Alan Turing first posed the question of whether machines can think. Today, AI is transforming societies and economies. It promises to generate productivity gains, improve well-being and help address global challenges, such as climate change, resource scarcity and health crises.\n",
      "Read more at https://play.google.com/store/books/details?id=eRmdDwAAQBAJ&source=gbs_api\n",
      "```\n",
      "\n",
      "## Issue \n",
      "\n",
      "This closes #27276 \n",
      "\n",
      "## Dependencies\n",
      "\n",
      "No additional dependencies were added\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: be3b7f9 - cookbook: add Anthropic's contextual retrieval (#27898)\n",
      "\n",
      "Hi there, this PR adds a notebook implementing Anthropic's proposed\n",
      "[Contextual\n",
      "retrieval](https://www.anthropic.com/news/contextual-retrieval) to\n",
      "langchain's cookbook.\n",
      "Commit: 733e43e - docs: new stack diagram (#27972)\n",
      "Commit: a073c4c - templates,docs: leave templates in v0.2 (#27952)\n",
      "\n",
      "all template installs will now have to declare `--branch v0.2` to make\n",
      "clear they aren't compatible with langchain 0.3 (most have a pydantic v1\n",
      "setup). e.g.\n",
      "\n",
      "```\n",
      "langchain-cli app add pirate-speak --branch v0.2\n",
      "```\n",
      "Commit: 8807e69 - docs: ignore case production fork master (#27971)\n",
      "Commit: 6f368e9 - community: handle chatdeepinfra jsondecode error (#27603)\n",
      "\n",
      "Fixes #27602 \n",
      "\n",
      "Added error handling to return empty dict if args is empty string or\n",
      "None.\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 0588bab - community: fix ClovaXEmbeddings document API link address (#27957)\n",
      "\n",
      "- **Description:** 404 error occurs because `API reference` link address\n",
      "path is incorrect on\n",
      "`langchain/docs/docs/integrations/text_embedding/naver.ipynb`\n",
      "- **Issue:** fix `API reference` link address correct path.\n",
      "\n",
      "@vbarda @efriis\n",
      "Commit: 05fd6a1 - Add ChatModels wrapper for Cloudflare Workers AI (#27645)\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- [x] **PR title**: \"community: chat models wrapper for Cloudflare\n",
      "Workers AI\"\n",
      "\n",
      "\n",
      "- [x] **PR message**:\n",
      "- **Description:** Add chat models wrapper for Cloudflare Workers AI.\n",
      "Enables Langgraph intergration via ChatModel for tool usage, agentic\n",
      "usage.\n",
      "\n",
      "\n",
      "- [x] **Add tests and docs**: If you're adding a new integration, please\n",
      "include\n",
      "1. a test for the integration, preferably unit tests that do not rely on\n",
      "network access,\n",
      "2. an example notebook showing its use. It lives in\n",
      "`docs/docs/integrations` directory.\n",
      "\n",
      "\n",
      "- [x] **Lint and test**: Run `make format`, `make lint` and `make test`\n",
      "from the root of the package(s) you've modified. See contribution\n",
      "guidelines for more: https://python.langchain.com/docs/contributing/\n",
      "\n",
      "Additional guidelines:\n",
      "- Make sure optional dependencies are imported within a function.\n",
      "- Please do not add dependencies to pyproject.toml files (even optional\n",
      "ones) unless they are required for unit tests.\n",
      "- Most PRs should not touch more than one package.\n",
      "- Changes should be backwards compatible.\n",
      "- If you are adding something to community, do not re-import it in\n",
      "langchain.\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Co-authored-by: Chester Curme <chester.curme@gmail.com>\n",
      "Commit: 8a5b9bf - box: migrate to repo (#27969)\n",
      "Commit: 1ad4995 - docs[patch]: update cassettes for sql/csv notebook (#27966)\n",
      "Commit: a747dbd - anthropic[patch]: remove retired model from tests (#27965)\n",
      "\n",
      "`claude-instant` was [retired\n",
      "yesterday](https://docs.anthropic.com/en/docs/resources/model-deprecations).\n",
      "Commit: 2cb3927 - community: bytes as a source to `AzureAIDocumentIntelligenceLoader` (#26618)\n",
      "\n",
      "- **Description:** This PR adds functionality to pass in in-memory bytes\n",
      "as a source to `AzureAIDocumentIntelligenceLoader`.\n",
      "- **Issue:** I needed the functionality, so I added it.\n",
      "- **Dependencies:** NA\n",
      "- **Twitter handle:** @akseljoonas if this is a big enough change :)\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Aksel Joonas Reedi <aksel@klippa.com>\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 7a9149f - community: ZeroxPDFLoader (#27800)\n",
      "\n",
      "# OCR-based PDF loader\n",
      "\n",
      "This implements [Zerox](https://github.com/getomni-ai/zerox) PDF\n",
      "document loader.\n",
      "Zerox utilizes simple but very powerful (even though slower and more\n",
      "costly) approach to parsing PDF documents: it converts PDF to series of\n",
      "images and passes it to a vision model requesting the contents in\n",
      "markdown.\n",
      "\n",
      "It is especially suitable for complex PDFs that are not parsed well by\n",
      "other alternatives.\n",
      "\n",
      "## Example use:\n",
      "```python\n",
      "from langchain_community.document_loaders.pdf import ZeroxPDFLoader\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"\" ## your-api-key\n",
      "\n",
      "model = \"gpt-4o-mini\" ## openai model\n",
      "pdf_url = \"https://assets.ctfassets.net/f1df9zr7wr1a/soP1fjvG1Wu66HJhu3FBS/034d6ca48edb119ae77dec5ce01a8612/OpenAI_Sacra_Teardown.pdf\"\n",
      "\n",
      "loader = ZeroxPDFLoader(file_path=pdf_url, model=model)\n",
      "docs = loader.load()\n",
      "```\n",
      "\n",
      "The Zerox library supports wide range of provides/models. See Zerox\n",
      "documentation for details.\n",
      "\n",
      "- **Dependencies:** `zerox`\n",
      "- **Twitter handle:** @martintriska1\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erickfriis@gmail.com>\n",
      "Commit: 53b0a99 - community: Memcached LLM Cache Integration (#27323)\n",
      "\n",
      "## Description\n",
      "This PR adds support for Memcached as a usable LLM model cache by adding\n",
      "the ```MemcachedCache``` implementation relying on the\n",
      "[pymemcache](https://github.com/pinterest/pymemcache) client.\n",
      "\n",
      "Unit test-wise, the new integration is generally covered under existing\n",
      "import testing. All new functionality depends on pymemcache if\n",
      "instantiated and used, so to comply with the other cache implementations\n",
      "the PR also adds optional integration tests for ```MemcachedCache```.\n",
      "\n",
      "Since this is a new integration, documentation is added for Memcached as\n",
      "an integration and as an LLM Cache.\n",
      "\n",
      "## Issue\n",
      "This PR closes #27275 which was originally raised as a discussion in\n",
      "#27035\n",
      "\n",
      "## Dependencies\n",
      "There are no new required dependencies for langchain, but\n",
      "[pymemcache](https://github.com/pinterest/pymemcache) is required to\n",
      "instantiate the new ```MemcachedCache```.\n",
      "\n",
      "## Example Usage\n",
      "```python3\n",
      "from langchain.globals import set_llm_cache\n",
      "from langchain_openai import OpenAI\n",
      "\n",
      "from langchain_community.cache import MemcachedCache\n",
      "from pymemcache.client.base import Client\n",
      "\n",
      "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", n=2, best_of=2)\n",
      "set_llm_cache(MemcachedCache(Client('localhost')))\n",
      "\n",
      "# The first time, it is not yet in cache, so it should take longer\n",
      "llm.invoke(\"Which city is the most crowded city in the USA?\")\n",
      "\n",
      "# The second time it is, so it goes faster\n",
      "llm.invoke(\"Which city is the most crowded city in the USA?\")\n",
      "```\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: cfff2a0 - community: Update UC toolkit documentation to use LangGraph APIs (#26778)\n",
      "\n",
      "- **Description:** Update UC toolkit documentation to show an example of\n",
      "using recommended LangGraph agent APIs before the existing LangChain\n",
      "AgentExecutor example. Tested by manually running the updated example\n",
      "notebook\n",
      "- **Dependencies:** No new dependencies\n",
      "\n",
      "---------\n",
      "\n",
      "Signed-off-by: Sid Murching <sid.murching@databricks.com>\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: c2072d9 - Improvement[Partner] Improve qdrant vector store (#27251)\n",
      "\n",
      "- Add static method decorator\n",
      "- Add args for api doc\n",
      "- Fix word spelling\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 81f7daa - community: add InfinityRerank (#27043)\n",
      "\n",
      "**Description:** \n",
      "\n",
      "- Add a Reranker for Infinity server.\n",
      "\n",
      "**Dependencies:** \n",
      "\n",
      "This wrapper uses\n",
      "[infinity_client](https://github.com/michaelfeil/infinity/tree/main/libs/client_infinity/infinity_client)\n",
      "to connect to an Infinity server.\n",
      "\n",
      "**Tests and docs**\n",
      "\n",
      "- integration test: test_infinity_rerank.py\n",
      "- example notebook: infinity_rerank.ipynb\n",
      "[here](https://github.com/baptiste-pasquier/langchain/blob/feat/infinity-rerank/docs/docs/integrations/document_transformers/infinity_rerank.ipynb)\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 2494deb - infra: remove google creds from release and integration test workflows (#27950)\n",
      "Commit: 90189f5 - community: Allow other than default parsers in SharePointLoader and OneDriveLoader (#27716)\n",
      "\n",
      "## What this PR does?\n",
      "\n",
      "### Currently `O365BaseLoader` (and consequently both derived loaders)\n",
      "are limited to `pdf`, `doc`, `docx` files.\n",
      "- **Solution: here we introduce _handlers_ attribute that allows for\n",
      "custom handlers to be passed in. This is done in _dict_ form:**\n",
      "\n",
      "**Example:**\n",
      "```python\n",
      "from langchain_community.document_loaders.parsers.documentloader_adapter import DocumentLoaderAsParser\n",
      "# PR for DocumentLoaderAsParser here: https://github.com/langchain-ai/langchain/pull/27749\n",
      "from langchain_community.document_loaders.excel import UnstructuredExcelLoader\n",
      "\n",
      "xlsx_parser = DocumentLoaderAsParser(UnstructuredExcelLoader, mode=\"paged\")\n",
      "\n",
      "# create dictionary mapping file types to handlers (parsers)\n",
      "handlers = {\n",
      "    \"doc\": MsWordParser()\n",
      "    \"pdf\": PDFMinerParser()\n",
      "    \"txt\": TextParser()\n",
      "    \"xlsx\": xlsx_parser\n",
      "}\n",
      "loader = SharePointLoader(document_library_id=\"...\",\n",
      "                            handlers=handlers # pass handlers to SharePointLoader\n",
      "                            )\n",
      "documents = loader.load()\n",
      "\n",
      "# works the same in OneDriveLoader\n",
      "loader = OneDriveLoader(document_library_id=\"...\",\n",
      "                            handlers=handlers\n",
      "                            )\n",
      "```\n",
      "This dictionary is then passed to `MimeTypeBasedParser` same as in the\n",
      "[current\n",
      "implementation](https://github.com/langchain-ai/langchain/blob/5a2cfb49e045988d290a1c7e3a0c589d6b371694/libs/community/langchain_community/document_loaders/parsers/registry.py#L13).\n",
      "\n",
      "\n",
      "### Currently `SharePointLoader` and `OneDriveLoader` are separate\n",
      "loaders that both inherit from `O365BaseLoader`\n",
      "However both of these implement the same functionality. The only\n",
      "differences are:\n",
      "- `SharePointLoader` requires argument `document_library_id` whereas\n",
      "`OneDriveLoader` requires `drive_id`. These are just different names for\n",
      "the same thing.\n",
      "  - `SharePointLoader` implements significantly more features.\n",
      "- **Solution: `OneDriveLoader` is replaced with an empty shell just\n",
      "renaming `drive_id` to `document_library_id` and inheriting from\n",
      "`SharePointLoader`**\n",
      "\n",
      "**Dependencies:** None\n",
      "**Twitter handle:** @martintriska1\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "Commit: 482c168 - langchain_core: add `file_type` option to make file type default as `png` (#27855)\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- [ ] **PR title**: \"package: description\"\n",
      "- Where \"package\" is whichever of langchain, community, core, etc. is\n",
      "being modified. Use \"docs: ...\" for purely docs changes, \"templates:\n",
      "...\" for template changes, \"infra: ...\" for CI changes.\n",
      "  - Example: \"community: add foobar LLM\"\n",
      "\n",
      "- [ ] **description**\n",
      "langchain_core.runnables.graph_mermaid.draw_mermaid_png calls this\n",
      "function, but the Mermaid API returns JPEG by default. To be consistent,\n",
      "add the option `file_type` with the default `png` type.\n",
      "\n",
      "- [ ] **Add tests and docs**: If you're adding a new integration, please\n",
      "include\n",
      "With this small change, I didn't add tests and docs.\n",
      "\n",
      "- [ ] **Lint and test**: Run `make format`, `make lint` and `make test`\n",
      "from the root of the package(s) you've modified. See contribution\n",
      "guidelines for more:\n",
      "One long sentence was divided into two.\n",
      "\n",
      "Additional guidelines:\n",
      "- Make sure optional dependencies are imported within a function.\n",
      "- Please do not add dependencies to pyproject.toml files (even optional\n",
      "ones) unless they are required for unit tests.\n",
      "- Most PRs should not touch more than one package.\n",
      "- Changes should be backwards compatible.\n",
      "- If you are adding something to community, do not re-import it in\n",
      "langchain.\n",
      "\n",
      "If no one reviews your PR within a few days, please @-mention one of\n",
      "baskaryan, efriis, eyurtsev, ccurme, vbarda, hwchase17.\n",
      "Commit: 0f85dea - langchain-huggingface: use separate kwargs for queries and docs (#27857)\n",
      "\n",
      "Now `encode_kwargs` used for both for documents and queries and this\n",
      "leads to wrong embeddings. E. g.:\n",
      "```python\n",
      "    model_kwargs = {\"device\": \"cuda\", \"trust_remote_code\": True}\n",
      "    encode_kwargs = {\"normalize_embeddings\": False, \"prompt_name\": \"s2p_query\"}\n",
      "\n",
      "    model = HuggingFaceEmbeddings(\n",
      "        model_name=\"dunzhang/stella_en_400M_v5\",\n",
      "        model_kwargs=model_kwargs,\n",
      "        encode_kwargs=encode_kwargs,\n",
      "    )\n",
      "\n",
      "    query_embedding = np.array(\n",
      "        model.embed_query(\"What are some ways to reduce stress?\",)\n",
      "    )\n",
      "    document_embedding = np.array(\n",
      "        model.embed_documents(\n",
      "            [\n",
      "                \"There are many effective ways to reduce stress. Some common techniques include deep breathing, meditation, and physical activity. Engaging in hobbies, spending time in nature, and connecting with loved ones can also help alleviate stress. Additionally, setting boundaries, practicing self-care, and learning to say no can prevent stress from building up.\",\n",
      "                \"Green tea has been consumed for centuries and is known for its potential health benefits. It contains antioxidants that may help protect the body against damage caused by free radicals. Regular consumption of green tea has been associated with improved heart health, enhanced cognitive function, and a reduced risk of certain types of cancer. The polyphenols in green tea may also have anti-inflammatory and weight loss properties.\",\n",
      "            ]\n",
      "        )\n",
      "    )\n",
      "    print(model._client.similarity(query_embedding, document_embedding)) # output: tensor([[0.8421, 0.3317]], dtype=torch.float64)\n",
      "```\n",
      "But from the [model\n",
      "card](https://huggingface.co/dunzhang/stella_en_400M_v5#sentence-transformers)\n",
      "expexted like this:\n",
      "```python\n",
      "    model_kwargs = {\"device\": \"cuda\", \"trust_remote_code\": True}\n",
      "    encode_kwargs = {\"normalize_embeddings\": False}\n",
      "    query_encode_kwargs = {\"normalize_embeddings\": False, \"prompt_name\": \"s2p_query\"}\n",
      "\n",
      "    model = HuggingFaceEmbeddings(\n",
      "        model_name=\"dunzhang/stella_en_400M_v5\",\n",
      "        model_kwargs=model_kwargs,\n",
      "        encode_kwargs=encode_kwargs,\n",
      "        query_encode_kwargs=query_encode_kwargs,\n",
      "    )\n",
      "\n",
      "    query_embedding = np.array(\n",
      "        model.embed_query(\"What are some ways to reduce stress?\", )\n",
      "    )\n",
      "    document_embedding = np.array(\n",
      "        model.embed_documents(\n",
      "            [\n",
      "                \"There are many effective ways to reduce stress. Some common techniques include deep breathing, meditation, and physical activity. Engaging in hobbies, spending time in nature, and connecting with loved ones can also help alleviate stress. Additionally, setting boundaries, practicing self-care, and learning to say no can prevent stress from building up.\",\n",
      "                \"Green tea has been consumed for centuries and is known for its potential health benefits. It contains antioxidants that may help protect the body against damage caused by free radicals. Regular consumption of green tea has been associated with improved heart health, enhanced cognitive function, and a reduced risk of certain types of cancer. The polyphenols in green tea may also have anti-inflammatory and weight loss properties.\",\n",
      "            ]\n",
      "        )\n",
      "    )\n",
      "    print(model._client.similarity(query_embedding, document_embedding)) # tensor([[0.8398, 0.2990]], dtype=torch.float64)\n",
      "```\n",
      "Commit: 60123be - docs: fix trim_messages docstring (#27948)\n",
      "Commit: 14f1827 - docs: Adding notebook for cdp agentkit toolkit (#27910)\n",
      "\n",
      "- **Description:** Adding in the first pass of documentation for the CDP\n",
      "Agentkit Toolkit\n",
      "    - **Issue:** N/a\n",
      "    - **Dependencies:** cdp-langchain\n",
      "    - **Twitter handle:** @CoinbaseDev\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Co-authored-by: John Peterson <john.peterson@coinbase.com>\n",
      "Commit: ea0ad91 - community: added Document.id support to opensearch vectorstore (#27945)\n",
      "\n",
      "Description:\n",
      "* Added support of Document.id on OpenSearch vector store\n",
      "* Added tests cases to match\n",
      "Commit: 75aa82f - docs: Completed sentence under the heading  \"Instantiating a Browser … (#27944)\n",
      "\n",
      "…Toolkit\" in \"playwright.ipynb\" integration.\n",
      "\n",
      "- Completed the incomplete sentence in the Langchain Playwright\n",
      "documentation.\n",
      "\n",
      "- Enhanced documentation clarity to guide users on best practices for\n",
      "instantiating browser instances with Langchain Playwright.\n",
      "\n",
      "Example before:\n",
      "> \"It's always recommended to instantiate using the from_browser method\n",
      "so that the\n",
      "\n",
      "Example after:\n",
      "> \"It's always recommended to instantiate using the `from_browser`\n",
      "method so that the browser context is properly initialized and managed,\n",
      "ensuring seamless interaction and resource optimization.\"\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 67ce05a - core[patch]: make oai tool description optional (#27756)\n",
      "Commit: b2da311 - docs: document init_chat_model standard params (#27812)\n",
      "Commit: 395674d - community: re-arrange function call message parse logic for Qianfan (#27935)\n",
      "\n",
      "the [PR](https://github.com/langchain-ai/langchain/pull/26208) two month\n",
      "ago has a potential bug which causes malfunction of `tool_call` for\n",
      "`QianfanChatEndpoint` waiting for fix\n",
      "Commit: 41b7a51 - infra: starter codeowners file (#27929)\n",
      "Commit: 66966a6 - openai[patch]: release 0.2.6 (#27924)\n",
      "\n",
      "Some additions in support of [predicted\n",
      "outputs](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs)\n",
      "feature:\n",
      "- Bump openai sdk version\n",
      "- Add integration test\n",
      "- Add example to integration docs\n",
      "\n",
      "The `prediction` kwarg is already plumbed through model invocation.\n",
      "Commit: a8c473e - standard-tests: ci pipeline (#27923)\n",
      "Commit: c3b7556 - infra: release note grep order of operations (#27922)\n",
      "Commit: b3c8135 - infra: release note compute 2 (#27921)\n",
      "Commit: bff2a8b - standard-tests: add tools standard tests (#27899)\n",
      "Commit: f6b2f82 - community: chroma error patch(attribute changed on chroma) (#27827)\n",
      "\n",
      "There was a change of attribute name which was \"max_batch_size\". It's\n",
      "now \"get_max_batch_size\" method.\n",
      "I want to use \"create_batches\" which is right down below.\n",
      "\n",
      "Please check this PR link.\n",
      "reference: https://github.com/chroma-core/chroma/pull/2305\n",
      "\n",
      "---------\n",
      "\n",
      "Signed-off-by: Prithvi Kannan <prithvi.kannan@databricks.com>\n",
      "Co-authored-by: Prithvi Kannan <46332835+prithvikannan@users.noreply.github.com>\n",
      "Co-authored-by: Bagatur <22008038+baskaryan@users.noreply.github.com>\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Co-authored-by: Jun Yamog <jkyamog@gmail.com>\n",
      "Co-authored-by: Bagatur <baskaryan@gmail.com>\n",
      "Co-authored-by: ono-hiroki <86904208+ono-hiroki@users.noreply.github.com>\n",
      "Co-authored-by: Dobiichi-Origami <56953648+Dobiichi-Origami@users.noreply.github.com>\n",
      "Co-authored-by: Chester Curme <chester.curme@gmail.com>\n",
      "Co-authored-by: Duy Huynh <vndee.huynh@gmail.com>\n",
      "Co-authored-by: Rashmi Pawar <168514198+raspawar@users.noreply.github.com>\n",
      "Co-authored-by: sifatj <26035630+sifatj@users.noreply.github.com>\n",
      "Co-authored-by: Eric Pinzur <2641606+epinzur@users.noreply.github.com>\n",
      "Co-authored-by: Daniel Vu Dao <danielvdao@users.noreply.github.com>\n",
      "Co-authored-by: Ofer Mendelevitch <ofermend@gmail.com>\n",
      "Co-authored-by: Stéphane Philippart <wildagsx@gmail.com>\n",
      "Commit: a3bbbe6 - update llm graph transformer documentation (#27905)\n",
      "Commit: 31f4fb7 - standard-tests: release 0.3.0 (#27900)\n",
      "Commit: ba5cba0 - infra: get min versions (#27896)\n",
      "Commit: 6973f72 - docs: sidebar capitalization (#27894)\n",
      "Commit: 4b8cd7a - community: ✨ Use new OVHcloud batch embedding (#26209)\n",
      "\n",
      "- **Description:** change to do the batch embedding server side and not\n",
      "client side\n",
      "- **Twitter handle:** @wildagsx\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: ccurme <chester.curme@gmail.com>\n",
      "Commit: a54f390 - infra: fix prev tag output (#27892)\n",
      "Commit: 75f80c2 - infra: fix prev tag condition (#27891)\n",
      "Commit: d7c39e6 - community: update Vectara integration (#27869)\n",
      "\n",
      "Thank you for contributing to LangChain!\n",
      "\n",
      "- **Description:** Updated Vectara integration\n",
      "- **Issue:** refresh on descriptions across all demos and added UDF\n",
      "reranker\n",
      "- **Dependencies:** None\n",
      "- **Twitter handle:** @ofermend\n",
      "\n",
      "---------\n",
      "\n",
      "Co-authored-by: Bagatur <baskaryan@gmail.com>\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: 14a71a6 - infra: fix prev tag calculation (#27890)\n",
      "Commit: 5745f3b - docs: Update `messages.mdx` (#27856)\n",
      "\n",
      "### Description\n",
      "Updates phrasing for the header of the `Messages` section.\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Commit: e02a5ee - docs: Update VectorStore as_retriever method url in qa_chat_history_how_to.ipynb (#27844)\n",
      "\n",
      "**Description**: Update VectorStore `as_retriever` method api reference\n",
      "url in `qa_chat_history_how_to.ipynb`\n",
      "\n",
      "Co-authored-by: Erick Friis <erick@langchain.dev>\n",
      "Issue #13076: how to use reranker model with langchain in retrievalQA case?\n",
      "Issue #22898: [E:onnxruntime:Default, env.cc:228 ThreadMain] pthread_setaffinity_np failed for thread: 8353, index: 0, mask: {1, }, error code: 22 error msg: Invalid argument. Specify the number of threads explicitly so the affinity is not set.\n",
      "Issue #5387: Failure when running pip install langchain[all]\n",
      "Issue #14948: The calculated score is wrong when using DistanceStrategy.MAX_INNER_PRODUCT (Faiss)\n",
      "Issue #28096: docs: update \"quickstart\" tutorial\n",
      "Issue #27991: core[patch]: support numpy 2\n",
      "Issue #25176: OpenAIModerationChain doesn't handle API key correctly for openai>=v1.0.0\n",
      "Issue #23515: DOC: <Issue related to /v0.2/docs/integrations/document_loaders/rst/>\n",
      "Issue #22487: LangChain Conversation Looping with Itself After Initial Greeting\n",
      "Issue #22296: Standardize ChatModel docstrings and integration docs\n",
      "Issue #21610: ConversationSummaryBufferMemory does not work as expected with MongoDBChatMessageHistory\n",
      "Issue #20178: RFC: add `bind_tools` to BaseLanguageModel\n",
      "Issue #28099: Update llm_chain.ipynb\n",
      "Issue #28100: docs: Fixed missing spaces and rephrased the langserve portion\n",
      "Issue #28102: docs: Add hyperlink to immediately show the table at the bottom of th…\n",
      "Issue #24703: ChatOllama is missing the parameters seed and base_url\n",
      "Issue #18254: Issue: Error in LangChainTracer.on_chain_error callback: TracerException('No indexed run ID da9518a3-b6ce-4525-a985-10b3ad41fef9.')\n",
      "Issue #25697: OllamaEmbeddings cannot be configured for external provider adding base_url and auth as documentation said\n",
      "Issue #27851: Set api_key param to litellm client based on providers api keys\n",
      "Issue #27981: English Update and fixed a duplicate \"the\"\n",
      "Issue #27773: docs: throw on broken anchors\n",
      "Issue #28088: docs: Updated link to ensure reference to the correct header for ToolNode\n",
      "Issue #28089: docs: add docs on StrOutputParser\n",
      "Issue #27036: text-splitters[minor]: Ability to Passing a Tokenizer directly to TokenTextSplitter and updating `from_tiktoken_encoder`, `from_huggingface_tokenizer`\n",
      "Issue #27473: [Community]: Converted `GoogleApiClient` and `GoogleApiYoutubeLoader` to Python Class rather than DataClass to avoid unexpected errors \n",
      "Issue #28094: Inconsistencies in how-to guides titles were fixed.\n",
      "Issue #28092: xai[patch]: update core\n",
      "Issue #27415: docs: Add example using `TypedDict` in structured outputs how-to guide\n",
      "Issue #25760: Takes care of Open Issue: #25380\n",
      "Issue #27159: community: chore warn deprecate the tracer\n",
      "Issue #27927: Return only Output in LLMChain\n",
      "Issue #28070: core: release 0.3.18\n",
      "Issue #28069: core: added DeleteResponse to the module\n",
      "Issue #25147: Redis cache update() does not overwrite existing key/prompt\n",
      "Issue #25141: Complex request to Elasticsearch provide \"langchain_core.exceptions.OutputParserException: Parsing text\" when using \"from langchain.retrievers.self_query.base import SelfQueryRetriever\"\n",
      "Issue #28073: core[patch]: Update doc-strings in callbacks\n",
      "Issue #22061: KeyError: 'tail_type' when using LLMGraphTransformer\n",
      "Issue #28075: docs: fix spelling error\n",
      "Issue #28079: docs: Fixed broken link for AI models introduction\n",
      "Issue #28084: Proofreading and Editing Report for Migration Guide\n",
      "Issue #28087: docs: Fix missing space between the words API Reference\n",
      "Issue #27733: DOCS: Concept Section Improvements & Updates\n",
      "Issue #27889: fixed issue #1559\n",
      "Issue #28082: docs: Fixed additional 'the' and remove 'turns' to make explanation clearer\n",
      "Issue #28080: docs: Fixed broken link for Cloudfare docs for the models available\n",
      "Issue #28083: docs: Fixed wrong link redirect from JS ToolMessage to Python ToolMes…\n",
      "Issue #28078: docs: Fixed broken link to the Luminous model family introduction\n",
      "Issue #28085: Community: add needle retriever & document loader \n",
      "Issue #28051: DOC: Fix typo in documentation for streaming modes, correcting “witten” to “written” in “Emit custom output witten using LangGraph’s StreamWriter.”\n",
      "Issue #19356: ChatLiteLLMRouter ignores specified model selection (overrides it by taking the 1st)\n",
      "Issue #14832: Add an OpentelemetryTracer in LangChain\n",
      "Issue #24019: OllamaFunctions returning ValueError: `tool_calls` missing from AIMessage: {message}\n",
      "Issue #4945: Issue: Stream a response from LangChain's OpenAI with Pyton Flask API\n",
      "Issue #27933: partners: fix for hallucination of OllamaLLM when using image inputs\n",
      "Issue #7632: Dimension from embeddings\n",
      "Issue #28067: xai[patch]: update dependencies\n",
      "Issue #27914: Handled empty search result handling and updated the notebook\n",
      "Issue #28065: anthropic[patch]: add examples to API ref\n",
      "Issue #28032: partners: add xAI chat integration\n",
      "Issue #28063: anthropic[major]: release 0.3.0\n",
      "Issue #28062: openai[patch]: release 0.2.8\n",
      "Issue #28061: community[patch]: release 0.3.7\n",
      "Issue #28060: core[patch]: release 0.3.17\n",
      "Issue #27916: anthropic[major]: support python 3.13\n",
      "Issue #28034: docs: Fix typo in Tavily Search example\n",
      "Issue #28038: Improvement[Community]Improve Embeddings API\n",
      "Issue #14180: Add new Document Loader for JIRA\n",
      "Issue #28057: community: release 0.2.19\n",
      "Issue #28050: community: patch graphqa chains (CVE-2024-8309)\n",
      "Issue #28055: Update streaming.mdx\n",
      "Issue #28052: DOC: Fix typo in documentation for streaming modes, correcting 'witte…\n",
      "Issue #20442: Depreciated initialisation (v3) of Weaviate Vector Database client\n",
      "Issue #27942: (rfc): store tools on chat model\n",
      "Issue #11980: how to connect to sql view in database  \n",
      "Issue #25120: SelfQueryRetriever alloowed operators does not allow contain Chroma\n",
      "Issue #25119: Instantiating GPT4AllEmbeddings with no gpt4all_kwargs argument raises a ValidationError\n",
      "Issue #25099: DOC: Guide for Implementing Tool Calling with Message History in LangChain\n",
      "Issue #23626: BUG in langchain_community.vectorstores.qdrant\n",
      "Issue #28049: community: patch all graph qa chains\n",
      "Issue #28048: openai[patch]: fix azure oai stream check\n",
      "Issue #28047: openai[patch]: Release 0.2.7\n",
      "Issue #28046: community[patch]: Release 0.3.6\n",
      "Issue #28045: core[patch]: Release 0.3.16\n",
      "Issue #9980: Chroma - retrieve all documents\n",
      "Issue #19933: HuggingFacePipeline does not use chat template\n",
      "Issue #27024: Fixes: community: fix LanceDB return no metadata\n",
      "Issue #9848: How to connect MS-SQL with LANG-CHAIN\n",
      "Issue #27930: community[docs]: modify parameter for the LoRA adapter on the vllm page\n",
      "Issue #26884: Error from using ChromaDB - ValueError: Could not connect to tenant default_tenant. Are you sure it exists?\n",
      "Issue #18365: similarity_score_threshold isn't working for MongoDB Atlas Vector Search\n",
      "Issue #28007: Update tutorials.mdx\n",
      "Issue #28020: docs: Makes the phrasing more smooth and reasoning more clear\n",
      "Issue #28006: docs:Fixed missing hyperlink and changed AI to LLMs for clarity\n",
      "Issue #28001: docs: removed bolding from header\n",
      "Issue #28014: Correcting AzureOpenAI initialization\n",
      "Issue #28026: docs : Update sql_qa.ipynb\n",
      "Issue #27066: fix:Specified the model that solves langchain moonshotChat exmaple di…\n",
      "Issue #27001: added lxml feature\n",
      "Issue #26742: community: Remove backticks from query executed by SQL Database Agent if they are there\n",
      "Issue #26622: Community: Integrate Zyte's document loader\n",
      "PR #28096: docs: update \"quickstart\" tutorial\n",
      "PR #27991: core[patch]: support numpy 2\n",
      "PR #28099: Update llm_chain.ipynb\n",
      "PR #28100: docs: Fixed missing spaces and rephrased the langserve portion\n",
      "PR #28102: docs: Add hyperlink to immediately show the table at the bottom of th…\n",
      "PR #27851: Set api_key param to litellm client based on providers api keys\n",
      "PR #27981: English Update and fixed a duplicate \"the\"\n",
      "PR #27773: docs: throw on broken anchors\n",
      "PR #28088: docs: Updated link to ensure reference to the correct header for ToolNode\n",
      "PR #28089: docs: add docs on StrOutputParser\n",
      "PR #27036: text-splitters[minor]: Ability to Passing a Tokenizer directly to TokenTextSplitter and updating `from_tiktoken_encoder`, `from_huggingface_tokenizer`\n",
      "PR #27473: [Community]: Converted `GoogleApiClient` and `GoogleApiYoutubeLoader` to Python Class rather than DataClass to avoid unexpected errors \n",
      "PR #28094: Inconsistencies in how-to guides titles were fixed.\n",
      "PR #28092: xai[patch]: update core\n",
      "PR #27415: docs: Add example using `TypedDict` in structured outputs how-to guide\n",
      "PR #25760: Takes care of Open Issue: #25380\n",
      "PR #27159: community: chore warn deprecate the tracer\n",
      "PR #28070: core: release 0.3.18\n",
      "PR #28069: core: added DeleteResponse to the module\n",
      "PR #28073: core[patch]: Update doc-strings in callbacks\n",
      "PR #28075: docs: fix spelling error\n",
      "PR #28079: docs: Fixed broken link for AI models introduction\n",
      "PR #28084: Proofreading and Editing Report for Migration Guide\n",
      "PR #28087: docs: Fix missing space between the words API Reference\n",
      "PR #27733: DOCS: Concept Section Improvements & Updates\n",
      "PR #27889: fixed issue #1559\n",
      "PR #28082: docs: Fixed additional 'the' and remove 'turns' to make explanation clearer\n",
      "PR #28080: docs: Fixed broken link for Cloudfare docs for the models available\n",
      "PR #28083: docs: Fixed wrong link redirect from JS ToolMessage to Python ToolMes…\n",
      "PR #28078: docs: Fixed broken link to the Luminous model family introduction\n",
      "PR #28085: Community: add needle retriever & document loader \n",
      "PR #27933: partners: fix for hallucination of OllamaLLM when using image inputs\n",
      "PR #28067: xai[patch]: update dependencies\n",
      "PR #27914: Handled empty search result handling and updated the notebook\n",
      "PR #28065: anthropic[patch]: add examples to API ref\n",
      "PR #28032: partners: add xAI chat integration\n",
      "PR #28063: anthropic[major]: release 0.3.0\n",
      "PR #28062: openai[patch]: release 0.2.8\n",
      "PR #28061: community[patch]: release 0.3.7\n",
      "PR #28060: core[patch]: release 0.3.17\n",
      "PR #27916: anthropic[major]: support python 3.13\n",
      "PR #28034: docs: Fix typo in Tavily Search example\n",
      "PR #28038: Improvement[Community]Improve Embeddings API\n",
      "PR #28057: community: release 0.2.19\n",
      "PR #28050: community: patch graphqa chains (CVE-2024-8309)\n",
      "PR #28055: Update streaming.mdx\n",
      "PR #28052: DOC: Fix typo in documentation for streaming modes, correcting 'witte…\n",
      "PR #27942: (rfc): store tools on chat model\n",
      "PR #28049: community: patch all graph qa chains\n",
      "PR #28048: openai[patch]: fix azure oai stream check\n",
      "PR #28047: openai[patch]: Release 0.2.7\n",
      "PR #28046: community[patch]: Release 0.3.6\n",
      "PR #28045: core[patch]: Release 0.3.16\n",
      "PR #27024: Fixes: community: fix LanceDB return no metadata\n",
      "PR #27930: community[docs]: modify parameter for the LoRA adapter on the vllm page\n",
      "PR #28007: Update tutorials.mdx\n",
      "PR #28020: docs: Makes the phrasing more smooth and reasoning more clear\n",
      "PR #28006: docs:Fixed missing hyperlink and changed AI to LLMs for clarity\n",
      "PR #28001: docs: removed bolding from header\n",
      "PR #28014: Correcting AzureOpenAI initialization\n",
      "PR #28026: docs : Update sql_qa.ipynb\n",
      "PR #27066: fix:Specified the model that solves langchain moonshotChat exmaple di…\n",
      "PR #27001: added lxml feature\n",
      "PR #26742: community: Remove backticks from query executed by SQL Database Agent if they are there\n",
      "PR #26622: Community: Integrate Zyte's document loader\n",
      "PR #26601: community: arxiv and pubmed tools description fix\n",
      "PR #26251: Switch graph - FalkorDB\n",
      "PR #26203: community: Reflection output parser\n",
      "PR #26114: langchain_community.vectorstores.opensearch_vector_search: Close connection for async calls\n",
      "PR #25926: Feat: Cache text only option\n",
      "PR #22348: community: add SearchResourceEncryptionKey support\n",
      "PR #22120: Yandex search api wraper\n",
      "PR #22012: Integration of Cambridge Semantics AnzoGraph DB in LangChain community (graphs and chains)\n",
      "PR #21943: Lmstudio embed\n",
      "PR #21911: community : Update url_playwright from document_loaders to maximize efficiency retrieval\n",
      "PR #21857: TigerGraph CoPilot Support\n",
      "PR #21843: LMStudioLLM class added. It may not be perfect but it is working. All…\n",
      "PR #21709: langchain.vectorstores.azuresearch: Allow AzureSerach Vector Store to update index on schema changes\n",
      "PR #21680: community: Terminal tool\n",
      "PR #26635: Add pagination to be able to retrieve all the documents\n",
      "PR #21573: community: add preload support to Ollama LLM\n",
      "PR #21566: community: Add Support for Loading Documents from Alibaba Cloud OSS file.\n",
      "PR #21553: community: add support for sentence window retrieval in Qdrant.py by enabling integer ids when using Qdrant.from_texts(....)\n",
      "PR #21525: token usage(from rest and point) addition in stream and format_response_payload in 'azureml_endpoint/AzureMLChatOnlineEndpoint'\n",
      "PR #21515: community: updated pubmed wrapper\n",
      "PR #21503: community: add BytesIO support to PdfLoader\n",
      "PR #20430: community: update QuipLoader to support return text instead of html format\n",
      "PR #20689: community: Add HuggingFace encoder-only model for embedding\n",
      "PR #20027: community[patch]: Invoke callback prior to yielding token #16913\n",
      "PR #19921: community: [VespaStore] change to feed_iterable for the latest pyvespa version, and delete the function \"delete\"\n",
      "PR #19809: infra[minor]: added falkordb container to docker compose for integration tests\n",
      "PR #16433: community: Enhanced Python Segmenter with Signature Extraction\n",
      "PR #16210: [community]: improve office 365 loader\n",
      "PR #16031: community: add StreamingAPIGatewayWebSocketCallbackHandler\n",
      "PR #14249: Make LogFileLoader more generic\n",
      "PR #14105: Added an IMDb toolkit\n",
      "PR #13731: DB-bound (or, \"Convertor-based\") prompt templates + Cassandra and Feast implementation\n",
      "PR #12484: Fix poetry dependency resolution in devcontainer and simplify setup\n",
      "PR #11108: [Feat] Add ChatModel, LLM, and Embeddings for ERNIE Bot APIs \n",
      "PR #10973: Support for computing embeddings using ONNX \n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "github_client = GitHubClient(token=os.getenv(\"GITHUB_TOKEN\"))\n",
    "\n",
    "# 获取特定时间范围的更新\n",
    "updates = github_client.fetch_updates(\n",
    "    repo=\"langchain-ai/langchain\",\n",
    "    since=\"2024-11-01T00:00:00Z\",\n",
    "    until=\"2024-11-15T23:59:59Z\"\n",
    ")\n",
    "\n",
    "# 处理结果\n",
    "for commit in updates['commits']:\n",
    "    print(f\"Commit: {commit['sha'][:7]} - {commit['commit']['message']}\")\n",
    "\n",
    "for issue in updates['issues']:\n",
    "    print(f\"Issue #{issue['number']}: {issue['title']}\")\n",
    "\n",
    "for pr in updates['pull_requests']:\n",
    "    print(f\"PR #{pr['number']}: {pr['title']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
